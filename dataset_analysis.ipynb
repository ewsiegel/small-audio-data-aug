{"cells":[{"cell_type":"markdown","metadata":{"id":"LBceuKiKlwJZ"},"source":["# Setup"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VigrgPklwJc","executionInfo":{"status":"ok","timestamp":1733722695072,"user_tz":300,"elapsed":4558,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}},"outputId":"5e10e92d-1246-403e-b842-0d5daea9b46f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: dropbox in /usr/local/lib/python3.10/dist-packages (12.0.2)\n","Requirement already satisfied: requests>=2.16.2 in /usr/local/lib/python3.10/dist-packages (from dropbox) (2.32.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from dropbox) (1.16.0)\n","Requirement already satisfied: stone<3.3.3,>=2 in /usr/local/lib/python3.10/dist-packages (from dropbox) (3.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2024.8.30)\n","Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.10/dist-packages (from stone<3.3.3,>=2->dropbox) (3.11)\n"]}],"source":["import pandas as pd\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model\n","from pathlib import Path\n","import os\n","from tqdm import tqdm\n","import torch\n","import soundfile as sf\n","import numpy as np\n","import torchaudio"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqpGN8PFyfR7","executionInfo":{"status":"ok","timestamp":1733720819704,"user_tz":300,"elapsed":279,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}},"outputId":"c7a49ec1-5936-4da4-caa5-df3d90cdd29f"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["## Embedding Pipeline (Skip if already done)"],"metadata":{"id":"aIwKxwfU9WaJ"}},{"cell_type":"code","execution_count":61,"metadata":{"id":"6OobCNdelwJe","executionInfo":{"status":"ok","timestamp":1733722434205,"user_tz":300,"elapsed":517,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}}},"outputs":[],"source":["# control train dataset is in data/train/train.csv\n","# test dataset is in data/test/test.csv\n","# eval dataset is in data/eval/eval.csv\n","# real train dataset is in data/train_small/train_small.csv\n","# synthetic train dataset is in data/synthetic_data/synthetic_data.csv\n","# alternative big dataset tbd\n","\n","# load all datasets\n","dfs = {\n","    \"train\": pd.read_csv('data/train/train.csv'),\n","    \"test\": pd.read_csv('data/test/test.csv'),\n","    \"eval\": pd.read_csv('data/eval/eval.csv'),\n","    \"train_small\": pd.read_csv('data/train_small/train_small.csv'),\n","    \"synthetic_data\": pd.read_csv('data/synthetic_data/synthetic_data.csv')\n","}\n","\n","dirs = {\n","  \"train\": \"data/train/\",\n","  \"test\": \"data/test/\",\n","  \"eval\": \"data/eval/\",\n","  \"train_small\": \"data/train_small/\",\n","  \"synthetic_data\": \"data/synthetic_data/\"\n","}"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"LX0tO06GlwJe","executionInfo":{"status":"ok","timestamp":1733721331557,"user_tz":300,"elapsed":3483,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}}},"outputs":[],"source":["# Load pretrained model and processor\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n","wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)"]},{"cell_type":"code","source":["def read_audio(waveforms_obj, enforce_mono=True):\n","    \"\"\"General audio loading, based on a custom notation.\n","\n","    Expected use case is in conjunction with Datasets\n","    specified by JSON.\n","\n","    The parameter may just be a path to a file:\n","    `read_audio(\"/path/to/wav1.wav\")`\n","\n","    Alternatively, you can specify more options in a dict, e.g.:\n","    ```\n","    # load a file from sample 8000 through 15999\n","    read_audio({\n","        \"file\": \"/path/to/wav2.wav\",\n","        \"start\": 8000,\n","        \"stop\": 16000\n","    })\n","    ```\n","\n","    Which codecs are supported depends on your torchaudio backend.\n","    Refer to `torchaudio.load` documentation for further details.\n","\n","    Arguments\n","    ---------\n","    waveforms_obj : str, dict\n","        Path to audio or dict with the desired configuration.\n","\n","        Keys for the dict variant:\n","        - `\"file\"` (str): Path to the audio file.\n","        - `\"start\"` (int, optional): The first sample to load.\n","        If unspecified, load from the very first frame.\n","        - `\"stop\"` (int, optional): The last sample to load (exclusive).\n","        If unspecified or equal to start, load from `start` to the end.\n","        Will not fail if `stop` is past the sample count of the file and will\n","        return less frames.\n","    enforce_mono : bool, optional\n","        If True, convert multi-channel audio to mono by averaging across channels.\n","        Defaults to True.\n","\n","    Returns\n","    -------\n","    torch.Tensor\n","        1-channel: audio tensor with shape: `(samples, )`.\n","        >=2-channels: audio tensor with shape: `(samples, channels)`.\n","\n","    Example\n","    -------\n","    >>> dummywav = torch.rand(16000)\n","    >>> import os\n","    >>> tmpfile = str(getfixture('tmpdir') / \"wave.wav\")\n","    >>> write_audio(tmpfile, dummywav, 16000)\n","    >>> asr_example = { \"wav\": tmpfile, \"spk_id\": \"foo\", \"words\": \"foo bar\"}\n","    >>> loaded = read_audio(asr_example[\"wav\"])\n","    >>> loaded.allclose(dummywav.squeeze(0),atol=1e-4) # replace with eq with sox_io backend\n","    True\n","    \"\"\"\n","    if isinstance(waveforms_obj, str):\n","        audio, _ = torchaudio.load(waveforms_obj)\n","    else:\n","        path = waveforms_obj[\"file\"]\n","        start = waveforms_obj.get(\"start\", 0)\n","        # To match past SB behavior, `start == stop` or omitted `stop` means to\n","        # load all frames from `start` to the file end.\n","        stop = waveforms_obj.get(\"stop\", start)\n","\n","        if start < 0:\n","            raise ValueError(\n","                f\"Invalid sample range (start < 0): {start}..{stop}!\"\n","            )\n","\n","        if stop < start:\n","            # Could occur if the user tried one of two things:\n","            # - specify a negative value as an attempt to index from the end;\n","            # - specify -1 as an attempt to load up to the last sample.\n","            raise ValueError(\n","                f\"Invalid sample range (stop < start): {start}..{stop}!\\n\"\n","                'Hint: Omit \"stop\" if you want to read to the end of file.'\n","            )\n","\n","        # Requested to load until a specific frame?\n","        if start != stop:\n","            num_frames = stop - start\n","            audio, fs = torchaudio.load(\n","                path, num_frames=num_frames, frame_offset=start\n","            )\n","        else:\n","            # Load to the end.\n","            audio, fs = torchaudio.load(path, frame_offset=start)\n","\n","    # Convert multi-channel audio to mono by averaging across channels if needed\n","    if audio.shape[0] > 1 and enforce_mono:\n","        audio = torch.mean(audio, dim=0, keepdim=True)\n","\n","    audio = audio.transpose(0, 1)\n","    speech = audio.squeeze(1)\n","\n","    # Resample audio to 16kHz if necessary\n","    _, sample_rate = sf.read(waveforms_obj)\n","    if sample_rate != 16000:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n","        speech = resampler(torch.tensor(speech, dtype=torch.float32))  # Ensure float32 type\n","    else:\n","        speech = torch.tensor(speech, dtype=torch.float32)  # Ensure float32 type\n","\n","    return speech"],"metadata":{"id":"hpxMXYd9wByt","executionInfo":{"status":"ok","timestamp":1733721041438,"user_tz":300,"elapsed":1017,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["def get_wav2vec_embedding(input):\n","    # Preprocess the audio\n","    inputs = processor(input, sampling_rate=16000, return_tensors=\"pt\", padding=True).to(device)\n","\n","    # Extract embeddings\n","    with torch.no_grad():\n","        outputs = wav2vec(**inputs)\n","        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling to get fixed-size embedding\n","\n","    return embeddings"],"metadata":{"id":"ScXgheMFpsRr","executionInfo":{"status":"ok","timestamp":1733721462093,"user_tz":300,"elapsed":1024,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["def process(df, dir_path=\"\", device=\"cpu\"):\n","    num_files = len(df)\n","\n","    # Initialize an empty list to store embeddings\n","    all_embeddings = []\n","\n","    for index in tqdm(range(num_files), desc=\"Processing files\"):\n","        # Read and process a single audio file\n","        file_path = dir_path + df.loc[index, 'filename']\n","        input_audio = torch.tensor(read_audio(file_path), dtype=torch.float32).to(device)\n","\n","        # Pass the audio to Wav2Vec model\n","        with torch.no_grad():\n","            embedding = get_wav2vec_embedding(input_audio)  # Assuming it returns a single embedding tensor\n","\n","        # Append the result to the list\n","        all_embeddings.append(embedding.cpu().numpy())\n","\n","    # Convert list of embeddings to a PyTorch tensor\n","    all_embeddings = torch.tensor(all_embeddings, dtype=torch.float32, device=device)\n","\n","    # Normalize all embeddings (mean = 0, variance = 1) across all dimensions\n","    mean = all_embeddings.mean(dim=0, keepdim=True)\n","    std = all_embeddings.std(dim=0, keepdim=True) + 1e-8  # Avoid division by zero\n","    all_embeddings = (all_embeddings - mean) / std\n","\n","    # Move embeddings back to CPU and write to DataFrame\n","    df['embedding'] = all_embeddings.cpu().numpy().tolist()\n","\n","    return df"],"metadata":{"id":"2nZ3HGoCyTFR","executionInfo":{"status":"ok","timestamp":1733721715038,"user_tz":300,"elapsed":300,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fg_tzk2klwJf","executionInfo":{"status":"ok","timestamp":1733723546180,"user_tz":300,"elapsed":59280,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}},"outputId":"5feb934f-ce0f-4513-d490-c516bc5e2112"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rProcessing files:   0%|          | 0/1046 [00:00<?, ?it/s]<ipython-input-50-9cc8cf7ca350>:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  speech = resampler(torch.tensor(speech, dtype=torch.float32))  # Ensure float32 type\n","<ipython-input-56-8e77bdc94165>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_audio = torch.tensor(read_audio(file_path), dtype=torch.float32).to(device)\n","Processing files: 100%|██████████| 1046/1046 [00:57<00:00, 18.19it/s]\n"]}],"source":["# for all wav files in each dataset, get the embeddings for each dataset\n","#normalize the mean and variance of the embeddings for each dataset so that the mean is 0 and the variance is 1\n","\n","for key in dfs:\n","    df = dfs[key]\n","    dir_path = dirs[key]\n","    processed_df = process(df, dir_path, device)\n","    processed_df.to_csv(f'data/{key}/{key}_embeddings.csv', index=False)\n","    dfs[key] = processed_df\n",""]},{"cell_type":"markdown","source":["## Load Embedding CSVs (if embedding pipeline already ran)"],"metadata":{"id":"tkYBamVH9cxE"}},{"cell_type":"code","source":["for key in dfs:\n","    dfs[key] = pd.read_csv(f'data/{key}/{key}_embeddings.csv')"],"metadata":{"id":"AXA05RyI-AKK","executionInfo":{"status":"ok","timestamp":1733723622732,"user_tz":300,"elapsed":1268,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fy1iS4WBlwJh"},"source":["# Information Theoretical Analysis"]},{"cell_type":"markdown","metadata":{"id":"q13inAdVlwJh"},"source":["## Avoiding Estimating Probability Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwwoP8j8lwJh"},"outputs":[],"source":["# compute MMD between the embeddings of each dataset\n","def gaussian_kernel(x, y, sigma=1.0):\n","    \"\"\"\n","    Compute the Gaussian kernel between x and y.\n","\n","    Args:\n","        x (np.ndarray): Array of shape (n_samples, embedding_dim).\n","        y (np.ndarray): Array of shape (m_samples, embedding_dim).\n","        sigma (float): Bandwidth of the Gaussian kernel.\n","\n","    Returns:\n","        np.ndarray: Kernel matrix of shape (n_samples, m_samples).\n","    \"\"\"\n","    pairwise_dists = cdist(x, y, 'sqeuclidean')  # Squared Euclidean distances\n","    return np.exp(-pairwise_dists / (2 * sigma ** 2))\n","\n","def compute_mmd(X, Y, sigma=1.0):\n","    \"\"\"\n","    Compute the Maximum Mean Discrepancy (MMD) between two distributions.\n","\n","    Args:\n","        X (np.ndarray): Samples from the first distribution (n_samples, embedding_dim).\n","        Y (np.ndarray): Samples from the second distribution (m_samples, embedding_dim).\n","        sigma (float): Bandwidth of the Gaussian kernel.\n","\n","    Returns:\n","        float: MMD^2 value.\n","    \"\"\"\n","    n, m = len(X), len(Y)\n","\n","    # Compute kernel matrices\n","    K_xx = gaussian_kernel(X, X, sigma)\n","    K_yy = gaussian_kernel(Y, Y, sigma)\n","    K_xy = gaussian_kernel(X, Y, sigma)\n","\n","    # Compute MMD^2\n","    mmd = (\n","        np.sum(K_xx) / (n * (n - 1))  # Exclude diagonal for unbiased estimate\n","        + np.sum(K_yy) / (m * (m - 1))\n","        - 2 * np.sum(K_xy) / (n * m)\n","    )\n","\n","    return mmd\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeEMfJkTlwJh"},"outputs":[],"source":["# Kolmogorov-Smirnov test between the embeddings of each dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFeAUEVOlwJi"},"outputs":[],"source":["# Wasserstein distance between the embeddings of each dataset"]},{"cell_type":"markdown","metadata":{"id":"pFNGEiqZlwJi"},"source":["## Estimating Probability Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtE7GQ1YlwJi"},"outputs":[],"source":["# Jensen-Shannon divergence between the embeddings of each dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suG_LCJblwJi"},"outputs":[],"source":["# Bhattacharyya distance between the embeddings of each dataset"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.7"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}