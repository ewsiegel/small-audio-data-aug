{"cells":[{"cell_type":"markdown","metadata":{"id":"LBceuKiKlwJZ"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4558,"status":"ok","timestamp":1733722695072,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"0VigrgPklwJc","outputId":"5e10e92d-1246-403e-b842-0d5daea9b46f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ethan/.cache/pypoetry/virtualenvs/small-audio-data-aug-0m_dPsVX-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model\n","from pathlib import Path\n","import os\n","from tqdm import tqdm\n","import torch\n","import soundfile as sf\n","import numpy as np\n","import torchaudio"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1733720819704,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"FqpGN8PFyfR7","outputId":"c7a49ec1-5936-4da4-caa5-df3d90cdd29f"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{"id":"aIwKxwfU9WaJ"},"source":["## Embedding Pipeline (Skip if already done)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1733722434205,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"6OobCNdelwJe"},"outputs":[],"source":["# control train dataset is in data/train/train.csv\n","# test dataset is in data/test/test.csv\n","# eval dataset is in data/eval/eval.csv\n","# real train dataset is in data/train_small/train_small.csv\n","# synthetic train dataset is in data/synthetic_data/synthetic_data.csv\n","# alternative big dataset tbd\n","\n","# load all datasets\n","dfs = {\n","    #\"train\": pd.read_csv('data/train/train.csv'),\n","    #\"test\": pd.read_csv('data/test/test.csv'),\n","    #\"eval\": pd.read_csv('data/eval/eval.csv'),\n","    #\"train_small\": pd.read_csv('data/train_small/train_small.csv'),\n","    #\"synthetic_data\": pd.read_csv('data/synthetic_data/synthetic_data.csv')\n","    \"iemocap\": pd.read_csv('data/iemocap/iemocap.csv')\n","}\n","\n","dirs = {\n","  \"train\": \"data/train/\",\n","  \"test\": \"data/test/\",\n","  \"eval\": \"data/eval/\",\n","  \"train_small\": \"data/train_small/\",\n","  \"synthetic_data\": \"data/synthetic_data/\",\n","  \"iemocap\": \"data/iemocap/\"\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3483,"status":"ok","timestamp":1733721331557,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"LX0tO06GlwJe"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ethan/.cache/pypoetry/virtualenvs/small-audio-data-aug-0m_dPsVX-py3.12/lib/python3.12/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n"]}],"source":["# Load pretrained model and processor\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n","wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1017,"status":"ok","timestamp":1733721041438,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"hpxMXYd9wByt"},"outputs":[],"source":["def read_audio(waveforms_obj, enforce_mono=True):\n","    \"\"\"General audio loading, based on a custom notation.\n","\n","    Expected use case is in conjunction with Datasets\n","    specified by JSON.\n","\n","    The parameter may just be a path to a file:\n","    `read_audio(\"/path/to/wav1.wav\")`\n","\n","    Alternatively, you can specify more options in a dict, e.g.:\n","    ```\n","    # load a file from sample 8000 through 15999\n","    read_audio({\n","        \"file\": \"/path/to/wav2.wav\",\n","        \"start\": 8000,\n","        \"stop\": 16000\n","    })\n","    ```\n","\n","    Which codecs are supported depends on your torchaudio backend.\n","    Refer to `torchaudio.load` documentation for further details.\n","\n","    Arguments\n","    ---------\n","    waveforms_obj : str, dict\n","        Path to audio or dict with the desired configuration.\n","\n","        Keys for the dict variant:\n","        - `\"file\"` (str): Path to the audio file.\n","        - `\"start\"` (int, optional): The first sample to load.\n","        If unspecified, load from the very first frame.\n","        - `\"stop\"` (int, optional): The last sample to load (exclusive).\n","        If unspecified or equal to start, load from `start` to the end.\n","        Will not fail if `stop` is past the sample count of the file and will\n","        return less frames.\n","    enforce_mono : bool, optional\n","        If True, convert multi-channel audio to mono by averaging across channels.\n","        Defaults to True.\n","\n","    Returns\n","    -------\n","    torch.Tensor\n","        1-channel: audio tensor with shape: `(samples, )`.\n","        >=2-channels: audio tensor with shape: `(samples, channels)`.\n","\n","    Example\n","    -------\n","    >>> dummywav = torch.rand(16000)\n","    >>> import os\n","    >>> tmpfile = str(getfixture('tmpdir') / \"wave.wav\")\n","    >>> write_audio(tmpfile, dummywav, 16000)\n","    >>> asr_example = { \"wav\": tmpfile, \"spk_id\": \"foo\", \"words\": \"foo bar\"}\n","    >>> loaded = read_audio(asr_example[\"wav\"])\n","    >>> loaded.allclose(dummywav.squeeze(0),atol=1e-4) # replace with eq with sox_io backend\n","    True\n","    \"\"\"\n","    if isinstance(waveforms_obj, str):\n","        audio, _ = torchaudio.load(waveforms_obj)\n","    else:\n","        path = waveforms_obj[\"file\"]\n","        start = waveforms_obj.get(\"start\", 0)\n","        # To match past SB behavior, `start == stop` or omitted `stop` means to\n","        # load all frames from `start` to the file end.\n","        stop = waveforms_obj.get(\"stop\", start)\n","\n","        if start < 0:\n","            raise ValueError(\n","                f\"Invalid sample range (start < 0): {start}..{stop}!\"\n","            )\n","\n","        if stop < start:\n","            # Could occur if the user tried one of two things:\n","            # - specify a negative value as an attempt to index from the end;\n","            # - specify -1 as an attempt to load up to the last sample.\n","            raise ValueError(\n","                f\"Invalid sample range (stop < start): {start}..{stop}!\\n\"\n","                'Hint: Omit \"stop\" if you want to read to the end of file.'\n","            )\n","\n","        # Requested to load until a specific frame?\n","        if start != stop:\n","            num_frames = stop - start\n","            audio, fs = torchaudio.load(\n","                path, num_frames=num_frames, frame_offset=start\n","            )\n","        else:\n","            # Load to the end.\n","            audio, fs = torchaudio.load(path, frame_offset=start)\n","\n","    # Convert multi-channel audio to mono by averaging across channels if needed\n","    if audio.shape[0] > 1 and enforce_mono:\n","        audio = torch.mean(audio, dim=0, keepdim=True)\n","\n","    audio = audio.transpose(0, 1)\n","    speech = audio.squeeze(1)\n","\n","    # Resample audio to 16kHz if necessary\n","    _, sample_rate = sf.read(waveforms_obj)\n","    if sample_rate != 16000:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n","        speech = resampler(torch.tensor(speech, dtype=torch.float32))  # Ensure float32 type\n","    else:\n","        speech = torch.tensor(speech, dtype=torch.float32)  # Ensure float32 type\n","\n","    return speech"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1024,"status":"ok","timestamp":1733721462093,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"ScXgheMFpsRr"},"outputs":[],"source":["def get_wav2vec_embedding(input):\n","    # Preprocess the audio\n","    inputs = processor(input, sampling_rate=16000, return_tensors=\"pt\", padding=True).to(device)\n","\n","    # Extract embeddings\n","    with torch.no_grad():\n","        outputs = wav2vec(**inputs)\n","        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling to get fixed-size embedding\n","\n","    return embeddings"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":300,"status":"ok","timestamp":1733721715038,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"2nZ3HGoCyTFR"},"outputs":[],"source":["def process(df, dir_path=\"\", device=\"cpu\"):\n","    num_files = len(df)\n","\n","    # Initialize an empty list to store embeddings\n","    all_embeddings = []\n","\n","    for index in tqdm(range(num_files), desc=\"Processing files\"):\n","        # Read and process a single audio file\n","        file_path = dir_path + df.loc[index, 'filename']\n","        input_audio = torch.tensor(read_audio(file_path), dtype=torch.float32).to(device)\n","\n","        # Pass the audio to Wav2Vec model\n","        with torch.no_grad():\n","            embedding = get_wav2vec_embedding(input_audio)  # Assuming it returns a single embedding tensor\n","\n","        # Append the result to the list\n","        all_embeddings.append(embedding.cpu().numpy())\n","\n","    # Convert list of embeddings to a PyTorch tensor\n","    all_embeddings = torch.tensor(all_embeddings, dtype=torch.float32, device=device)\n","\n","    # Normalize all embeddings (mean = 0, variance = 1) across all dimensions\n","    mean = all_embeddings.mean(dim=0, keepdim=True)\n","    std = all_embeddings.std(dim=0, keepdim=True) + 1e-8  # Avoid division by zero\n","    all_embeddings = (all_embeddings - mean) / std\n","\n","    # Move embeddings back to CPU and write to DataFrame\n","    df['embedding'] = all_embeddings.cpu().numpy().tolist()\n","\n","    return df"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59280,"status":"ok","timestamp":1733723546180,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"fg_tzk2klwJf","outputId":"5feb934f-ce0f-4513-d490-c516bc5e2112"},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing files:   0%|          | 0/4597 [00:00<?, ?it/s]/tmp/ipykernel_43573/2613402436.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  speech = torch.tensor(speech, dtype=torch.float32)  # Ensure float32 type\n","/tmp/ipykernel_43573/1465984915.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_audio = torch.tensor(read_audio(file_path), dtype=torch.float32).to(device)\n","Processing files: 100%|██████████| 4597/4597 [02:06<00:00, 36.46it/s]\n","/tmp/ipykernel_43573/1465984915.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  all_embeddings = torch.tensor(all_embeddings, dtype=torch.float32, device=device)\n"]}],"source":["# for all wav files in each dataset, get the embeddings for each dataset\n","#normalize the mean and variance of the embeddings for each dataset so that the mean is 0 and the variance is 1\n","\n","for key in dfs:\n","    df = dfs[key]\n","    dir_path = dirs[key]\n","    processed_df = process(df, dir_path, device)\n","    processed_df.to_csv(f'{key}_embeddings.csv', index=False)\n","    dfs[key] = processed_df\n"]},{"cell_type":"markdown","metadata":{"id":"tkYBamVH9cxE"},"source":["## Load Embedding CSVs (if embedding pipeline already ran)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1268,"status":"ok","timestamp":1733723622732,"user":{"displayName":"Ethan Siegel","userId":"01172728029829569749"},"user_tz":300},"id":"AXA05RyI-AKK"},"outputs":[{"ename":"KeyError","evalue":"'train'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dfs:\n\u001b[1;32m      2\u001b[0m     dfs[key] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_embeddings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n","\u001b[0;31mKeyError\u001b[0m: 'train'"]}],"source":["for key in dfs:\n","    dfs[key] = pd.read_csv(f'{key}_embeddings.csv')\n","\n","dfs[\"train\"]"]},{"cell_type":"markdown","metadata":{"id":"Fy1iS4WBlwJh"},"source":["# Information Theoretical Analysis"]},{"cell_type":"markdown","metadata":{"id":"q13inAdVlwJh"},"source":["## Avoiding Estimating Probability Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwwoP8j8lwJh"},"outputs":[],"source":["# compute MMD between the embeddings of each dataset\n","def gaussian_kernel(x, y, sigma=1.0):\n","    \"\"\"\n","    Compute the Gaussian kernel between x and y.\n","\n","    Args:\n","        x (np.ndarray): Array of shape (n_samples, embedding_dim).\n","        y (np.ndarray): Array of shape (m_samples, embedding_dim).\n","        sigma (float): Bandwidth of the Gaussian kernel.\n","\n","    Returns:\n","        np.ndarray: Kernel matrix of shape (n_samples, m_samples).\n","    \"\"\"\n","    pairwise_dists = cdist(x, y, 'sqeuclidean')  # Squared Euclidean distances\n","    return np.exp(-pairwise_dists / (2 * sigma ** 2))\n","\n","def compute_mmd(X, Y, sigma=1.0):\n","    \"\"\"\n","    Compute the Maximum Mean Discrepancy (MMD) between two distributions.\n","\n","    Args:\n","        X (np.ndarray): Samples from the first distribution (n_samples, embedding_dim).\n","        Y (np.ndarray): Samples from the second distribution (m_samples, embedding_dim).\n","        sigma (float): Bandwidth of the Gaussian kernel.\n","\n","    Returns:\n","        float: MMD^2 value.\n","    \"\"\"\n","    n, m = len(X), len(Y)\n","\n","    # Compute kernel matrices\n","    K_xx = gaussian_kernel(X, X, sigma)\n","    K_yy = gaussian_kernel(Y, Y, sigma)\n","    K_xy = gaussian_kernel(X, Y, sigma)\n","\n","    # Compute MMD^2\n","    mmd = (\n","        np.sum(K_xx) / (n * (n - 1))  # Exclude diagonal for unbiased estimate\n","        + np.sum(K_yy) / (m * (m - 1))\n","        - 2 * np.sum(K_xy) / (n * m)\n","    )\n","\n","    return mmd\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeEMfJkTlwJh"},"outputs":[],"source":["# Kolmogorov-Smirnov test between the embeddings of each dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFeAUEVOlwJi"},"outputs":[],"source":["# Wasserstein distance between the embeddings of each dataset"]},{"cell_type":"markdown","metadata":{"id":"pFNGEiqZlwJi"},"source":["## Estimating Probability Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtE7GQ1YlwJi"},"outputs":[],"source":["# Jensen-Shannon divergence between the embeddings of each dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suG_LCJblwJi"},"outputs":[],"source":["# Bhattacharyya distance between the embeddings of each dataset"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
