<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>The Platonic Representation Hypothesis</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Coping with Small Audio Data: A Theoretical and Practical Approach</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/ewsiegel">Ethan Siegel</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/drew-g-ross">Drew Ross</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/jerryylu48">Jerry Lu</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#solutions">Solutions</a><br><br>
              <a href="#implementation">Implementation</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#analysis">Analysis</a><br><br>
              <a href="#implications_and_limitations">Implications and Limitations</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/audiofileanalyzer.jpg" width=512px/>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Introduction</h2>
            <p>
				Audio data is a crucial component of many speech-oriented applications and modeling tasks, 
				but it is often difficult to obtain and process audio data that is robust, accurate, and representative for a downstream task. 
				Many existing methods for audio processing, especially modern deep learning-based approaches, are designed for large datasets, 
				but in practice, audio data is often limited in quantity and quality, especially since audio data is an inherently noisy and complex signal.
				This can lead to suboptimal performance and reduced accuracy in downstream tasks.
			</p>
			<p>
				Our goal is to explore and evaluate state-of-the-art approaches for training deep learning models for downstream audio tasks
				when the initial audio data is limited. To do this, we will focus on training deep learning models for a downstream
				speech emotion classification task. We will pay attention to the methods used to generate the training data and how it
				affects the performance of the model, and evaluate implications from a practical and theoretical perspective.
			</p>
			<p>
				Specifically, we will focus on the techniques of pre-training on a larger dataset and synthetic data generation
				as two practical approaches to handling the small audio data problem. Then, we will leverage the Wasserstein Distance
				(Earth Mover's Distance) between two datasets as a theoretical way to evaluate the expected performance of these
				approaches.
			</p>
			<p>
				In doing so, we aim to develop an understanding for handling small audio data
				for deep learning in a disciplined way, and advance methods to do larger scale machine learning on audio
				based modalities.
			</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="solutions">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h2>Solutions for Handling Small Audio Data: Practical and Theoretical Considerations</h2>
					<p>
						Let's break down our two main approaches to leveraging small audio data for deep learning tasks
						that require large amounts of data, when we only have a small dataset:
					</p>
					<h1>Method 1: Pre-training on a large similar dataset, and then fine-tuning on our small dataset.</h1>
					<p>
						This method involves finding a large dataset that is similar to our target task that our small dataset
						was collected from, pre-training for many epochs on this large dataset, and then fine-tuning on our small dataset
						for a few epochs.
					</p>
					<p>
						A successful implementation of this method would result in a robust general pretrained model that can be fine-tuned
						on our small dataset to successfully perform the downstream speech emotion classification task. This would require
						ample transfer learning from the large dataset, requiring that the large dataset is similar enough to our small dataset.
					</p>
					<p>
						The advantage of this method is that it is much more simple and straightforward to find existing data than
						generate or augment new data. However, the primary disadvantage is that the large dataset may not be exactly
						similar to our small dataset, and so the pretrained model may not be able to generalize as well to the downstream task.
					</p>
					<h1>Method 2: Generating synthetic data from our small dataset, pre-training on the synthetic data, and then fine-tuning on our small dataset.</h1>
					<p>
						This method involves generating synthetic data from our small dataset, pre-training for many epochs on the synthetic data,
						and then fine-tuning for a few epochs on our small dataset.
					</p>
					<p>
						A successful implementation of this method would involve generating a synthetic dataset that is similar to our small dataset,
						pre-training a task-specific model on the synthetic data, and then fine-tuning on our small dataset for precision.
					</p>
					<p>
						The advantage of this method is that if we can generate a synthetic dataset that is verysimilar to our small dataset,
						this would allow for much more accurate and generalizable training. However, the primary disadvantage is the difficulty
						in generating high quality synthetic data in the audio domain.
					</p>
					<h1>Theoretical Consideration: Quantifying Similarity Between Not Well-Aligned Datasets</h1>
					<p>
					<p>
						To quantify difference between our datasets, we had to consider that the datasets were not well-aligned, 
						meaning they had differences in scale, structure, or temporal alignment that required preprocessing for accurate comparison.
						At a higher level, the "Friends" dataset comprises conversations from the television show, featuring scripted interactions, 
						whereas the other datasets consist of spontaneous speech, characterized by informal language, 
						and varied contextual backgrounds. These inherent differences in language style, structure, and context 
						lead to distinct distributional properties, making direct comparisons and alignment challenging.
					</p>
					<p>
						Many statistical difference measures exist to quantify difference between two probability distributions (we can think of our datasets as 
						samples drawn from their respective probability distributions). 
						Among these, Jensen-Shannon (JS) Divergence, Bhattacharyya Distance, and Wasserstein Distance are prominent. 
						JS and Bhattacharyya require defined CDFs or PDFs, which aren't easily calculated for the high-dimensional fixed-embeddings generated from our datasets, making them less effective. 
						In contrast, Wasserstein Distance excels in handling high-dimensional, unaligned data. Therefore, we chose Wasserstein distance as our similarity metric.
						
					</p>
				    	<p>
						Wasserstein distance provides a theoretically rigorous way to measure the similarity between probability distributions. In our context, we can leverage
						this metric to quantify how "close" two audio datasets are in their embedding space. By computing the minimum
						"cost" of transforming the distribution of embeddings from one dataset into another,
						the Wasserstein distance gives us a meaningful measure of dataset similarity that accounts for both
						the location and shape of the distributions. This is particularly valuable when comparing our small target
						dataset with either a large pre-training dataset (Method 1) or synthetic data (Method 2), as it
						allows us to mathematically validate how well our chosen/generated data aligns with the target
						distribution. A smaller Wasserstein distance would suggest better potential for transfer learning
						or synthetic data quality, while larger distances may indicate fundamental distributional differences
						that could limit performance. 
				    	<p>
						In the context of our project, we first processed all audio samples through
						<a href="https://huggingface.co/facebook/wav2vec2-base">Facebook's wav2vec2 model</a> to obtain
						fixed-dimensional embeddings that capture the acoustic features of the speech. Using
						these embeddings as points in the high-dimensional space, we calculated the Wasserstein distance. This metric helped us understand
						which datasets were most similar to our small training set in terms of their underlying audio characteristics
						and validate our approach of using synthetic data and pre-training. 
					</p>
			    		<h1>Additional Design Choices</h1>
			    		<p>
						To calculate the Wasserstein distance between our datasets, we first need to establish a cost matrix that defines the "cost" of 
						moving data points from one distribution to another. This matrix is essential for quantifying the effort required to align the two datasets. 
						Upon analysis, we found that the embedding values ranged between -11 and 11: a relatively small distance that does not require normalization. 
						Therefore, we selected Euclidean distance as our cost metric for Wasserstein distance calculation due to its simplicity.
					</p>
				    	<p>
						We acknowledge that in high-dimensional datasets, dimensionality-reduction techniques such as Principle Component Analysis
						(PCA) are important for computational efficiency (both in model training and computing the Wasserstein distance) and noise reduction. 
						However, <a href="https://arxiv.org/pdf/2006.11477">prior</a> research using our chosen embedding dimension size has demonstrated sufficient results without dimensionality reduction.
					</p>
		
						
					</p>
					<p>
						This project will involve testing an implementation of these two methods, evaluating their performance,
						then analyzing the results from the mentioned theoretical perspective.
					</p>
			    		<br>
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		    </div>
		</div>

		<div class="content-margin-container" id="implementation">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
				<h2>Implementation</h2>
				<p>
					Here we will discuss the implementation of the two methods described in the previous section.
				</p>
				<h1>Data and Preprocessing</h1>
				<p>
					First, let's discuss the data we will be using. Our downstream task will be speech emotion classification
					on the <a href="https://affective-meld.github.io/">MELD dataset</a>, which contains around 15,000 utterances
					from the TV show Friends paired with their corresponding emotion labels. Thus, our downstream task is to
					predict the emotion of new utterances from Friends. For our purposes, we chose to only focus on
					datapoints labelled with the emotion labeles "neutral", "joy", "sadness", "anger", and "surprise".
				</p>
				<div style="text-align: center; margin: 20px 0;">
					<audio controls style="width: 100%;">
						<source src="images/example4.wav" type="audio/wav">
						Your browser does not support the audio element.
					</audio>
					<p style="margin-top: 5px; font-size: 14px;">Example MELD Audio Clip</p>
				</div>
				<p>
					After filtering the dataset to only include these labels and splitting into train/test/validation sets, we have
					roughly 9500 labelled utterances for training, 4500 for testing, and 1000 for validation. Additionally, to
					simulate having a small training dataset, we sampled 1 of every 4 training datapoints to create a small training
					dataset of roughly 2350 samples.
				</p>
				<p>
					The large train dataset will only be used as a baseline reference to evaluate the performance of our other
					experiments.

					TODO put table with all datasets and their sizes
				</p>
				<h1>Model for Downstream Task</h1>
				<p>
					TODO: paragraph on model for downstream task.
				</p>
				<h1>Method 1: Pre-training on a large similar dataset, and then fine-tuning on our small dataset.</h1>
				<p>
					For our large pre-training dataset that is similar to our Friends emotion classification task, we chose
					the <a href="https://sail.usc.edu/iemocap/">IEMOCAP dataset</a>, which contains around 7,500 utterances
					paired with its corresponding emotion label. The dataset is comprised of actor dialogues speaking English,
					similar to the Friends dataset.
				</p>
				<div style="text-align: center; margin: 20px 0;">
					<audio controls style="width: 100%;">
						<source src="images/example5.wav" type="audio/wav">
						Your browser does not support the audio element.
					</audio>
					<p style="margin-top: 5px; font-size: 14px;">Example IEMOCAP Audio Clip</p>
				</div>
				<p>
					TODO: paragraph on method for pretraining on iemocap. how many epochs, hyperparameters, etc.
				</p>
				<p></p>
					TODO: paragraph on fine-tuning on small dataset. how many epochs, hyperparameters, etc.
				</p>
				<h1>Method 2: Generating synthetic data from our small dataset, pre-training on the synthetic data, and then fine-tuning on our small dataset.</h1>
				<p>
					To generate synthetic data from our small dataset, we leveraged Microsoft's
					<a href="https://www.microsoft.com/en-us/research/project/e2-tts/">E2TTS</a> model, which is a text-to-speech
					model that can be used to generate speech from text, replicating the emotion of an inputted utterance.
					First, we used a LLM to generate a large amount of common English utterances in text. Next, for each utterance
					in the small dataset, we inputted the utterance and 5randomly selected LLM generated text utterances into
					the E2TTS model, and generated 5 new audio samples that are saying new text in the same emotion as the inputted
					utterance. This process allowed us to generate a synthetic dataset of around 12,000 samples whose distribution
					of emotions matched that of our small dataset. Below are some examples of synthetic utterances generated by E2TTS
					and the emotion of the utterance.
				</p>
				<div style="display: flex; justify-content: space-between; margin: 20px 0;">
					<div style="flex: 1; text-align: center; margin: 0 10px;">
						<audio controls style="width: 100%;">
							<source src="images/example1.wav" type="audio/wav">
							Your browser does not support the audio element.
						</audio>
						<p style="margin-top: 5px; font-size: 14px;">Joy</p>
					</div>
					<div style="flex: 1; text-align: center; margin: 0 10px;">
						<audio controls style="width: 100%;">
							<source src="images/example2.wav" type="audio/wav">
							Your browser does not support the audio element.
						</audio>
						<p style="margin-top: 5px; font-size: 14px;">Neutral</p>
					</div>
					<div style="flex: 1; text-align: center; margin: 0 10px;">
						<audio controls style="width: 100%;">
							<source src="images/example3.wav" type="audio/wav">
							Your browser does not support the audio element.
						</audio>
						<p style="margin-top: 5px; font-size: 14px;">Sadness</p>
					</div>
				</div>
				<p>
					TODO: paragraph on method for pretraining on synthetic data. how many epochs, hyperparameters, etc.
				</p>
				<p>
					TODO: paragraph on fine-tuning on small dataset. how many epochs, hyperparameters, etc.
				</p>

			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="results">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
				<h2>Results</h2>
				<h1>Test Results</h1>
				<table style="width: 100%; border-collapse: collapse; margin-top: 10px;">
					<tr>
						<th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Dataset</th>
						<th style="border: 1px solid #ddd; padding: 8px; text-align: left;"># Training Data Points</th>
						<th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Test Accuracy</th>
					</tr>
					<tr>
						<td style="border: 1px solid #ddd; padding: 8px;">full_MELD</td>
						<td style="border: 1px solid #ddd; padding: 8px;">9449</td>
						<td style="border: 1px solid #ddd; padding: 8px;">0.601</td>
					</tr>
					<tr>
						<td style="border: 1px solid #ddd; padding: 8px;">small_MELD</td>
						<td style="border: 1px solid #ddd; padding: 8px;">2353</td>
						<td style="border: 1px solid #ddd; padding: 8px;">0.510</td>
					</tr>
					<tr>
						<td style="border: 1px solid #ddd; padding: 8px;">IEMOCAP + small_MELD</td>
						<td style="border: 1px solid #ddd; padding: 8px;">6950</td>
						<td style="border: 1px solid #ddd; padding: 8px;">0.515</td>
					</tr>
					<tr>
						<td style="border: 1px solid #ddd; padding: 8px;">Synthetic + small_MELD</td>
						<td style="border: 1px solid #ddd; padding: 8px;">14118</td>
						<td style="border: 1px solid #ddd; padding: 8px;">0.535</td>
					</tr>
				</table>
				<h1>Wasserstein Distance</h1>
				<p>
					The Wasserstein distances between our small training dataset and each of the other datasets revealed an intuitive
					pattern that validates our approach. As shown in Figure 4, the large training dataset from MELD had the smallest
					Wasserstein distance to our small training set, which is expected since they come from the same source distribution.
					The test and evaluation sets (also from MELD) showed the next smallest distances, confirming the consistency of the
					MELD dataset's audio characteristics across splits. Interestingly, our synthetic dataset demonstrated a smaller
					Wasserstein distance to the small training set compared to the IEMOCAP dataset, which was used in our first
					pre-training approach. This quantitatively supports our hypothesis that synthetic data generation can produce
					samples that better match the target distribution compared to using a different real-world dataset. The larger
					distance to IEMOCAP suggests that while it contains emotional speech data, its underlying audio characteristics
					differ more substantially from our target MELD distribution, potentially limiting the effectiveness of transfer
					learning from this source.
				</p>
				<img src="images/wasserstein.png" width=512px style="display: block; margin: auto;"/>
				<p style="text-align: center; font-size: 14px;">Wasserstein distances between the small training dataset and other datasets, showing relative similarities in their underlying audio distributions.</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="analysis">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
				<h2>Analysis</h2>
				<p>
					TODO: paragraph on analysis of the results.
				</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
				<h2>Implications and Limitations</h2>
				<p>
					TODO: paragraph on implications and limitations.
				</p>
			    	<h2>Extensions</h2>
			    	<p>
					Incorporating Wasserstein distance into model fine-tuning: TODO
					Implementing a Wasserstein Generative Adversarial Network (WGAN): TODO
					
				</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
			<div class="margin-left-block">
			</div>
		    <div class="main-content-block">
				<div class='citation' id="references" style="height:auto"><br>
					<span style="font-size:16px">References:</span><br><br>
					<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
					<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
				</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
